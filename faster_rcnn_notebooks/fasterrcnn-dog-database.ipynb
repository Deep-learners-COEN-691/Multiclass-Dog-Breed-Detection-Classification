{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13356928,"sourceType":"datasetVersion","datasetId":8471832}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:12:17.983481Z","iopub.execute_input":"2025-10-12T21:12:17.983711Z","iopub.status.idle":"2025-10-12T21:12:19.610586Z","shell.execute_reply.started":"2025-10-12T21:12:17.983693Z","shell.execute_reply":"2025-10-12T21:12:19.609784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/stanford-dog-database\n!ls /kaggle/input/stanford-dog-database/images | head\n!ls /kaggle/input/stanford-dog-database/annotation | head\n!ls /kaggle/input/stanford-dog-database/annotation/Annotation/n02085620-Chihuahua | head\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:12:19.611267Z","iopub.execute_input":"2025-10-12T21:12:19.611546Z","iopub.status.idle":"2025-10-12T21:12:20.103140Z","shell.execute_reply.started":"2025-10-12T21:12:19.611530Z","shell.execute_reply":"2025-10-12T21:12:20.102457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1Ô∏è‚É£ Copy dataset from read-only input folder to writable working directory\n!cp -r /kaggle/input/stanford-dog-database /kaggle/working/dogs\n\n# 2Ô∏è‚É£ Verify the copy\n!ls /kaggle/working/dogs\n!ls /kaggle/working/dogs/annotation/Annotation | head\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:12:20.105404Z","iopub.execute_input":"2025-10-12T21:12:20.105638Z","iopub.status.idle":"2025-10-12T21:15:33.607950Z","shell.execute_reply.started":"2025-10-12T21:12:20.105618Z","shell.execute_reply":"2025-10-12T21:15:33.607189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\nann_root = Path(\"/kaggle/working/dogs/annotation/Annotation\")  # <-- change me\ncount = 0\nfor p in ann_root.rglob(\"*\"):\n    if p.is_file() and p.suffix == \"\":\n        try:\n            head = p.read_bytes()[:128].decode(\"utf-8\", errors=\"ignore\")\n            if \"<annotation\" in head:\n                new = p.with_name(p.name + \".xml\")\n                if not new.exists():\n                    p.rename(new)\n                    count += 1\n        except Exception:\n            pass\nprint(\"Renamed\", count, \"files to .xml\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:33.608948Z","iopub.execute_input":"2025-10-12T21:15:33.609241Z","iopub.status.idle":"2025-10-12T21:15:35.216426Z","shell.execute_reply.started":"2025-10-12T21:15:33.609195Z","shell.execute_reply":"2025-10-12T21:15:35.215748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/dogs/annotation/Annotation/n02085620-Chihuahua\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:35.217134Z","iopub.execute_input":"2025-10-12T21:15:35.217350Z","iopub.status.idle":"2025-10-12T21:15:35.338046Z","shell.execute_reply.started":"2025-10-12T21:15:35.217333Z","shell.execute_reply":"2025-10-12T21:15:35.337276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/dogs/images/Images/n02085620-Chihuahua","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:35.339027Z","iopub.execute_input":"2025-10-12T21:15:35.339324Z","iopub.status.idle":"2025-10-12T21:15:35.458620Z","shell.execute_reply.started":"2025-10-12T21:15:35.339303Z","shell.execute_reply":"2025-10-12T21:15:35.457941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nimg_root = Path(\"/kaggle/working/dogs/images/Images\")\nann_root = Path(\"/kaggle/working/dogs/annotation/Annotation\")\n\nmissing_xml, missing_jpg = [], []\n\n# loop over each breed folder\nfor breed in sorted(os.listdir(img_root)):\n    img_dir = img_root / breed\n    ann_dir = ann_root / breed\n    if not ann_dir.exists():\n        print(f\"‚ö†Ô∏è No annotation folder for {breed}\")\n        continue\n\n    # collect basenames (without extensions)\n    imgs = {p.stem for p in img_dir.glob(\"*.jpg\")}\n    xmls = {p.stem for p in ann_dir.glob(\"*.xml\")}\n\n    # compare\n    no_xml = imgs - xmls\n    no_img = xmls - imgs\n\n    if no_xml:\n        missing_xml.extend([(breed, n) for n in sorted(no_xml)])\n    if no_img:\n        missing_jpg.extend([(breed, n) for n in sorted(no_img)])\n\nprint(f\"\\n‚úÖ Breeds checked: {len(list(img_root.iterdir()))}\")\nprint(f\"üü• Images with no XML: {len(missing_xml)}\")\nprint(f\"üü¶ XMLs with no Image: {len(missing_jpg)}\")\n\n# Show first few mismatches for inspection\nprint(\"\\n--- First 10 images missing XML ---\")\nfor b, n in missing_xml[:10]:\n    print(f\"{b}/{n}.jpg\")\n\nprint(\"\\n--- First 10 XMLs missing Image ---\")\nfor b, n in missing_jpg[:10]:\n    print(f\"{b}/{n}.xml\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:35.459524Z","iopub.execute_input":"2025-10-12T21:15:35.459775Z","iopub.status.idle":"2025-10-12T21:15:35.562111Z","shell.execute_reply.started":"2025-10-12T21:15:35.459744Z","shell.execute_reply":"2025-10-12T21:15:35.561584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#EDA\n# ============================================================\n# Exploratory Data Analysis (EDA) for Stanford Dogs Dataset\n# for Faster R-CNN / RetinaNet / YOLOv8 Comparative Study\n# ============================================================\n\nimport os\nimport cv2\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom pathlib import Path\nimport random\n\n# ------------------------------------------------------------\n# 1Ô∏è‚É£ CONFIGURATION\n# ------------------------------------------------------------\nDATA_ROOT = \"/kaggle/working/dogs\"  # Adjust if different\nIMG_DIR = Path(DATA_ROOT) / \"images\" / \"Images\"\nANN_DIR = Path(DATA_ROOT) / \"annotation\" / \"Annotation\"\n\n# Directory sanity check\nprint(f\"Images path: {IMG_DIR.exists()}, Annotations path: {ANN_DIR.exists()}\")\n\n# ------------------------------------------------------------\n# 2Ô∏è‚É£ BASIC DATASET STATS\n# ------------------------------------------------------------\nbreeds = sorted([d for d in os.listdir(IMG_DIR) if os.path.isdir(IMG_DIR / d)])\nprint(f\"‚úÖ Total Breeds: {len(breeds)}\")\n\nbreed_counts = {}\nfor breed in breeds:\n    breed_counts[breed] = len(list((IMG_DIR / breed).glob(\"*.jpg\")))\n\ntotal_images = sum(breed_counts.values())\nprint(f\"‚úÖ Total Images: {total_images}\")\n\n# Convert to DataFrame for convenience\nbreed_df = pd.DataFrame(list(breed_counts.items()), columns=[\"Breed\", \"ImageCount\"])\nbreed_df[\"Breed\"] = breed_df[\"Breed\"].str.split(\"-\").str[-1]  # Cleaner labels\n\n# ------------------------------------------------------------\n# 3Ô∏è‚É£ CLASS DISTRIBUTION\n# ------------------------------------------------------------\nplt.figure(figsize=(18,5))\nsns.barplot(data=breed_df.sort_values(\"ImageCount\", ascending=False).head(30),\n            x=\"Breed\", y=\"ImageCount\", palette=\"viridis\")\nplt.xticks(rotation=90)\nplt.title(\"Top 30 Breeds by Image Count\")\nplt.ylabel(\"Number of Images\")\nplt.xlabel(\"Dog Breed\")\nplt.tight_layout()\nplt.show()\n\n# Class imbalance metric\nimbalance_ratio = breed_df[\"ImageCount\"].max() / breed_df[\"ImageCount\"].min()\nprint(f\"‚öñÔ∏è Class imbalance ratio: {imbalance_ratio:.2f}\")\n\n# ------------------------------------------------------------\n# 4Ô∏è‚É£ IMAGE RESOLUTION DISTRIBUTION\n# ------------------------------------------------------------\nimg_sizes = []\nfor breed in random.sample(breeds, 10):  # sample 10 breeds for speed\n    for img_path in (IMG_DIR / breed).glob(\"*.jpg\"):\n        img = cv2.imread(str(img_path))\n        if img is not None:\n            h, w = img.shape[:2]\n            img_sizes.append((w, h))\nimg_sizes = np.array(img_sizes)\n\nplt.figure(figsize=(6,6))\nplt.scatter(img_sizes[:,0], img_sizes[:,1], alpha=0.3, color=\"teal\")\nplt.xlabel(\"Width (px)\")\nplt.ylabel(\"Height (px)\")\nplt.title(\"Image Resolution Distribution (sample of 10 breeds)\")\nplt.grid(True)\nplt.show()\n\nprint(f\"Mean width: {img_sizes[:,0].mean():.1f}px, Mean height: {img_sizes[:,1].mean():.1f}px\")\n\n# ------------------------------------------------------------\n# 5Ô∏è‚É£ BOUNDING BOX GEOMETRY ANALYSIS\n# ------------------------------------------------------------\nareas, ratios = [], []\n\nfor breed in random.sample(breeds, 10):  # sample subset for faster analysis\n    for xml_file in (ANN_DIR / breed).glob(\"*.xml\"):\n        try:\n            root = ET.parse(xml_file).getroot()\n            for obj in root.findall(\"object\"):\n                bbox = obj.find(\"bndbox\")\n                xmin, ymin = int(bbox.find(\"xmin\").text), int(bbox.find(\"ymin\").text)\n                xmax, ymax = int(bbox.find(\"xmax\").text), int(bbox.find(\"ymax\").text)\n                w, h = xmax - xmin, ymax - ymin\n                if w > 0 and h > 0:\n                    areas.append(w * h)\n                    ratios.append(w / h)\n        except:\n            continue\n\n# Bounding box area distribution\nplt.figure(figsize=(6,4))\nsns.histplot(areas, bins=40, color=\"orange\", kde=True)\nplt.title(\"Bounding Box Area Distribution\")\nplt.xlabel(\"Area (pixels¬≤)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Bounding box aspect ratio\nplt.figure(figsize=(6,4))\nsns.histplot(ratios, bins=40, color=\"green\", kde=True)\nplt.title(\"Bounding Box Aspect Ratio Distribution (w/h)\")\nplt.xlabel(\"Aspect Ratio\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nprint(f\"Average box area: {np.mean(areas):.1f}, Average aspect ratio: {np.mean(ratios):.2f}\")\n\n# ------------------------------------------------------------\n# 6Ô∏è‚É£ SAMPLE VISUALIZATION WITH BOXES\n# ------------------------------------------------------------\ndef show_sample_with_boxes(breed=None):\n    if breed is None:\n        breed = random.choice(breeds)\n    img_files = list((IMG_DIR / breed).glob(\"*.jpg\"))\n    img_path = random.choice(img_files)\n    xml_path = ANN_DIR / breed / (Path(img_path).stem + \".xml\")\n\n    img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n    root = ET.parse(xml_path).getroot()\n    for obj in root.findall(\"object\"):\n        bbox = obj.find(\"bndbox\")\n        xmin, ymin = int(bbox.find(\"xmin\").text), int(bbox.find(\"ymin\").text)\n        xmax, ymax = int(bbox.find(\"xmax\").text), int(bbox.find(\"ymax\").text)\n        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n        cv2.putText(img, obj.find(\"name\").text, (xmin, ymin-5),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n\n    plt.figure(figsize=(6,6))\n    plt.imshow(img)\n    plt.title(f\"Sample: {breed}\")\n    plt.axis(\"off\")\n    plt.show()\n\n# Show random samples\nfor _ in range(3):\n    show_sample_with_boxes()\n\n# ------------------------------------------------------------\n# 7Ô∏è‚É£ SAVE METRICS SUMMARY\n# ------------------------------------------------------------\nsummary = {\n    \"Total Breeds\": len(breeds),\n    \"Total Images\": total_images,\n    \"Imbalance Ratio\": round(imbalance_ratio, 2),\n    \"Mean Image Width\": round(img_sizes[:,0].mean(), 1),\n    \"Mean Image Height\": round(img_sizes[:,1].mean(), 1),\n    \"Mean Box Area\": round(np.mean(areas), 1),\n    \"Mean Aspect Ratio\": round(np.mean(ratios), 2)\n}\n\nsummary_df = pd.DataFrame(list(summary.items()), columns=[\"Metric\", \"Value\"])\nprint(\"\\n=== EDA Summary ===\")\nprint(summary_df.to_string(index=False))\n\n# Optionally save summary as CSV for your report\nsummary_df.to_csv(\"eda_summary.csv\", index=False)\nprint(\"\\n‚úÖ EDA summary saved as eda_summary.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport xml.etree.ElementTree as ET\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass StanfordDogsDataset(Dataset):\n    def __init__(self, root_dir, transforms=None):\n        \"\"\"\n        Custom PyTorch Dataset for the Stanford Dogs dataset.\n\n        Args:\n            root_dir (str): Path to dataset root (contains 'images' and 'annotation' folders)\n            transforms (callable, optional): Transformations to apply to images\n        \"\"\"\n        self.root_dir = root_dir\n        self.img_dir = os.path.join(root_dir, \"images\", \"Images\")\n        self.ann_dir = os.path.join(root_dir, \"annotation\", \"Annotation\")\n        self.transforms = transforms\n\n        self.image_paths = []\n        self.annotation_paths = []\n\n        # --- Collect all image + XML annotation pairs ---\n        for breed_folder in os.listdir(self.img_dir):\n            img_folder = os.path.join(self.img_dir, breed_folder)\n            ann_folder = os.path.join(self.ann_dir, breed_folder)\n\n            if not os.path.isdir(img_folder) or not os.path.isdir(ann_folder):\n                continue\n\n            for img_file in os.listdir(img_folder):\n                if img_file.lower().endswith((\".jpg\", \".jpeg\")):\n                    base = os.path.splitext(img_file)[0]\n                    xml_file = os.path.join(ann_folder, base + \".xml\")\n\n                    if os.path.exists(xml_file):\n                        self.image_paths.append(os.path.join(img_folder, img_file))\n                        self.annotation_paths.append(xml_file)\n\n        # --- Build breed-to-index mapping (based on folder names) ---\n        self.classes = sorted(os.listdir(self.img_dir))\n        self.class_to_idx = {cls_name: i + 1 for i, cls_name in enumerate(self.classes)}  # +1 for background\n\n        print(f\"‚úÖ Dataset initialized: {len(self.image_paths)} image-annotation pairs found across {len(self.classes)} breeds.\")\n\n    def __len__(self):\n        \"\"\"Return total number of samples.\"\"\"\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        \"\"\"Load one image and its corresponding annotation.\"\"\"\n        img_path = self.image_paths[idx]\n        ann_path = self.annotation_paths[idx]\n\n        # --- Load image ---\n        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n\n        # --- Parse XML annotation ---\n        root = ET.parse(ann_path).getroot()\n        boxes, labels = [], []\n\n        # Handle known naming inconsistencies\n        corrections = {\"Pekinese\": \"Pekingese\"}\n\n        for obj in root.findall(\"object\"):\n            breed = obj.find(\"name\").text.strip()\n            breed = corrections.get(breed, breed)\n\n            # Folder name includes both WordNet ID and breed (e.g., n02086079-Pekingese)\n            folder_name = os.path.basename(os.path.dirname(img_path))\n\n            # Main label assignment\n            if folder_name in self.class_to_idx:\n                label = self.class_to_idx[folder_name]\n            elif breed in self.class_to_idx:\n                label = self.class_to_idx[breed]\n            else:\n                # Skip if breed not found\n                continue\n\n            bbox = obj.find(\"bndbox\")\n            xmin = int(bbox.find(\"xmin\").text)\n            ymin = int(bbox.find(\"ymin\").text)\n            xmax = int(bbox.find(\"xmax\").text)\n            ymax = int(bbox.find(\"ymax\").text)\n\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(label)\n\n        # Convert to tensors\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {\"boxes\": boxes, \"labels\": labels}\n\n        if self.transforms:\n            img = self.transforms(img)\n\n        return img, target\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:35.562805Z","iopub.execute_input":"2025-10-12T21:15:35.562993Z","iopub.status.idle":"2025-10-12T21:15:44.803723Z","shell.execute_reply.started":"2025-10-12T21:15:35.562974Z","shell.execute_reply":"2025-10-12T21:15:44.803132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.ToTensor()\nroot_path = \"/kaggle/working/dogs\"\ndataset = StanfordDogsDataset(root_path, transforms=transform)\nprint(\"‚úÖ Total samples:\", len(dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:44.805609Z","iopub.execute_input":"2025-10-12T21:15:44.805973Z","iopub.status.idle":"2025-10-12T21:15:44.965799Z","shell.execute_reply.started":"2025-10-12T21:15:44.805956Z","shell.execute_reply":"2025-10-12T21:15:44.965201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, target = dataset[0]\nprint(\"Image shape:\", img.shape)\nprint(\"Boxes:\", target[\"boxes\"])\nprint(\"Labels:\", target[\"labels\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:44.966488Z","iopub.execute_input":"2025-10-12T21:15:44.966723Z","iopub.status.idle":"2025-10-12T21:15:45.038159Z","shell.execute_reply.started":"2025-10-12T21:15:44.966699Z","shell.execute_reply":"2025-10-12T21:15:45.037455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\n\ntorch.manual_seed(42)\ntrain_size = int(0.8 * len(dataset))\ntest_size  = len(dataset) - train_size\ntrain_ds, test_ds = random_split(dataset, [train_size, test_size])\n\ntrain_dl = DataLoader(train_ds, batch_size=2, shuffle=True,\n                      collate_fn=lambda x: tuple(zip(*x)))\ntest_dl  = DataLoader(test_ds, batch_size=1, shuffle=False,\n                      collate_fn=lambda x: tuple(zip(*x)))\n\nprint(f\"Train: {len(train_ds)}  Test: {len(test_ds)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:45.038922Z","iopub.execute_input":"2025-10-12T21:15:45.039153Z","iopub.status.idle":"2025-10-12T21:15:45.052439Z","shell.execute_reply.started":"2025-10-12T21:15:45.039130Z","shell.execute_reply":"2025-10-12T21:15:45.051714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = fasterrcnn_resnet50_fpn(weights=\"COCO_V1\")\n\nnum_classes = len(dataset.classes) + 1   # +1 for background\nin_feats = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes)\n\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:45.053247Z","iopub.execute_input":"2025-10-12T21:15:45.053553Z","iopub.status.idle":"2025-10-12T21:15:47.084190Z","shell.execute_reply.started":"2025-10-12T21:15:45.053533Z","shell.execute_reply":"2025-10-12T21:15:47.083560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.Adam([p for p in model.parameters()\n                              if p.requires_grad], lr=5e-4)\n\nnum_epochs = 2  # start small to test\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (imgs, tgts) in enumerate(train_dl):\n        imgs  = [im.to(device) for im in imgs]\n        tgts  = [{k:v.to(device) for k,v in t.items()} for t in tgts]\n\n        loss_dict = model(imgs, tgts)\n        loss = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 10 == 0:\n            print(f\"Epoch[{epoch+1}/{num_epochs}] Step[{i}] Loss: {loss.item():.4f}\")\n    print(f\"Epoch[{epoch+1}] Avg Loss: {running_loss/len(train_dl):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:15:47.084916Z","iopub.execute_input":"2025-10-12T21:15:47.085159Z","iopub.status.idle":"2025-10-12T23:51:10.235571Z","shell.execute_reply.started":"2025-10-12T21:15:47.085142Z","shell.execute_reply":"2025-10-12T23:51:10.234923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Visualize predictions\nimport matplotlib.pyplot as plt\n\nmodel.eval()\nimg, _ = test_ds[0]\nwith torch.no_grad():\n    pred = model([img.to(device)])\n\nplt.imshow(img.permute(1,2,0))\nfor box, score, label in zip(pred[0][\"boxes\"], pred[0][\"scores\"], pred[0][\"labels\"]):\n    if score > 0.5:\n        x1,y1,x2,y2 = box\n        plt.gca().add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1,\n                                          fill=False, color=\"lime\", lw=2))\n        plt.text(x1, y1-5, f\"{dataset.classes[label-1]} ({score:.2f})\",\n                 color=\"white\", fontsize=8,\n                 bbox=dict(facecolor=\"lime\", alpha=0.5))\nplt.axis(\"off\"); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T23:52:55.968307Z","iopub.execute_input":"2025-10-12T23:52:55.968578Z","iopub.status.idle":"2025-10-12T23:52:56.385684Z","shell.execute_reply.started":"2025-10-12T23:52:55.968558Z","shell.execute_reply":"2025-10-12T23:52:56.384977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save\ntorch.save(model.state_dict(), \"/kaggle/working/fasterrcnn_dogs.pth\")\nprint(\"Model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T23:53:11.622002Z","iopub.execute_input":"2025-10-12T23:53:11.622735Z","iopub.status.idle":"2025-10-12T23:53:11.854501Z","shell.execute_reply.started":"2025-10-12T23:53:11.622709Z","shell.execute_reply":"2025-10-12T23:53:11.853715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#EDA\n# ============================================================\n# Exploratory Data Analysis (EDA) for Stanford Dogs Dataset\n# for Faster R-CNN / RetinaNet / YOLOv8 Comparative Study\n# ============================================================\n\nimport os\nimport cv2\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom pathlib import Path\nimport random\n\n# ------------------------------------------------------------\n# 1Ô∏è‚É£ CONFIGURATION\n# ------------------------------------------------------------\nDATA_ROOT = \"/kaggle/working/dogs\"  # Adjust if different\nIMG_DIR = Path(DATA_ROOT) / \"images\" / \"Images\"\nANN_DIR = Path(DATA_ROOT) / \"annotation\" / \"Annotation\"\n\n# Directory sanity check\nprint(f\"Images path: {IMG_DIR.exists()}, Annotations path: {ANN_DIR.exists()}\")\n\n# ------------------------------------------------------------\n# 2Ô∏è‚É£ BASIC DATASET STATS\n# ------------------------------------------------------------\nbreeds = sorted([d for d in os.listdir(IMG_DIR) if os.path.isdir(IMG_DIR / d)])\nprint(f\"‚úÖ Total Breeds: {len(breeds)}\")\n\nbreed_counts = {}\nfor breed in breeds:\n    breed_counts[breed] = len(list((IMG_DIR / breed).glob(\"*.jpg\")))\n\ntotal_images = sum(breed_counts.values())\nprint(f\"‚úÖ Total Images: {total_images}\")\n\n# Convert to DataFrame for convenience\nbreed_df = pd.DataFrame(list(breed_counts.items()), columns=[\"Breed\", \"ImageCount\"])\nbreed_df[\"Breed\"] = breed_df[\"Breed\"].str.split(\"-\").str[-1]  # Cleaner labels\n\n# ------------------------------------------------------------\n# 3Ô∏è‚É£ CLASS DISTRIBUTION\n# ------------------------------------------------------------\nplt.figure(figsize=(18,5))\nsns.barplot(data=breed_df.sort_values(\"ImageCount\", ascending=False).head(30),\n            x=\"Breed\", y=\"ImageCount\", palette=\"viridis\")\nplt.xticks(rotation=90)\nplt.title(\"Top 30 Breeds by Image Count\")\nplt.ylabel(\"Number of Images\")\nplt.xlabel(\"Dog Breed\")\nplt.tight_layout()\nplt.show()\n\n# Class imbalance metric\nimbalance_ratio = breed_df[\"ImageCount\"].max() / breed_df[\"ImageCount\"].min()\nprint(f\"‚öñÔ∏è Class imbalance ratio: {imbalance_ratio:.2f}\")\n\n# ------------------------------------------------------------\n# 4Ô∏è‚É£ IMAGE RESOLUTION DISTRIBUTION\n# ------------------------------------------------------------\nimg_sizes = []\nfor breed in random.sample(breeds, 10):  # sample 10 breeds for speed\n    for img_path in (IMG_DIR / breed).glob(\"*.jpg\"):\n        img = cv2.imread(str(img_path))\n        if img is not None:\n            h, w = img.shape[:2]\n            img_sizes.append((w, h))\nimg_sizes = np.array(img_sizes)\n\nplt.figure(figsize=(6,6))\nplt.scatter(img_sizes[:,0], img_sizes[:,1], alpha=0.3, color=\"teal\")\nplt.xlabel(\"Width (px)\")\nplt.ylabel(\"Height (px)\")\nplt.title(\"Image Resolution Distribution (sample of 10 breeds)\")\nplt.grid(True)\nplt.show()\n\nprint(f\"Mean width: {img_sizes[:,0].mean():.1f}px, Mean height: {img_sizes[:,1].mean():.1f}px\")\n\n# ------------------------------------------------------------\n# 5Ô∏è‚É£ BOUNDING BOX GEOMETRY ANALYSIS\n# ------------------------------------------------------------\nareas, ratios = [], []\n\nfor breed in random.sample(breeds, 10):  # sample subset for faster analysis\n    for xml_file in (ANN_DIR / breed).glob(\"*.xml\"):\n        try:\n            root = ET.parse(xml_file).getroot()\n            for obj in root.findall(\"object\"):\n                bbox = obj.find(\"bndbox\")\n                xmin, ymin = int(bbox.find(\"xmin\").text), int(bbox.find(\"ymin\").text)\n                xmax, ymax = int(bbox.find(\"xmax\").text), int(bbox.find(\"ymax\").text)\n                w, h = xmax - xmin, ymax - ymin\n                if w > 0 and h > 0:\n                    areas.append(w * h)\n                    ratios.append(w / h)\n        except:\n            continue\n\n# Bounding box area distribution\nplt.figure(figsize=(6,4))\nsns.histplot(areas, bins=40, color=\"orange\", kde=True)\nplt.title(\"Bounding Box Area Distribution\")\nplt.xlabel(\"Area (pixels¬≤)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Bounding box aspect ratio\nplt.figure(figsize=(6,4))\nsns.histplot(ratios, bins=40, color=\"green\", kde=True)\nplt.title(\"Bounding Box Aspect Ratio Distribution (w/h)\")\nplt.xlabel(\"Aspect Ratio\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nprint(f\"Average box area: {np.mean(areas):.1f}, Average aspect ratio: {np.mean(ratios):.2f}\")\n\n# ------------------------------------------------------------\n# 6Ô∏è‚É£ SAMPLE VISUALIZATION WITH BOXES\n# ------------------------------------------------------------\ndef show_sample_with_boxes(breed=None):\n    if breed is None:\n        breed = random.choice(breeds)\n    img_files = list((IMG_DIR / breed).glob(\"*.jpg\"))\n    img_path = random.choice(img_files)\n    xml_path = ANN_DIR / breed / (Path(img_path).stem + \".xml\")\n\n    img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n    root = ET.parse(xml_path).getroot()\n    for obj in root.findall(\"object\"):\n        bbox = obj.find(\"bndbox\")\n        xmin, ymin = int(bbox.find(\"xmin\").text), int(bbox.find(\"ymin\").text)\n        xmax, ymax = int(bbox.find(\"xmax\").text), int(bbox.find(\"ymax\").text)\n        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n        cv2.putText(img, obj.find(\"name\").text, (xmin, ymin-5),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n\n    plt.figure(figsize=(6,6))\n    plt.imshow(img)\n    plt.title(f\"Sample: {breed}\")\n    plt.axis(\"off\")\n    plt.show()\n\n# Show random samples\nfor _ in range(3):\n    show_sample_with_boxes()\n\n# ------------------------------------------------------------\n# 7Ô∏è‚É£ SAVE METRICS SUMMARY\n# ------------------------------------------------------------\nsummary = {\n    \"Total Breeds\": len(breeds),\n    \"Total Images\": total_images,\n    \"Imbalance Ratio\": round(imbalance_ratio, 2),\n    \"Mean Image Width\": round(img_sizes[:,0].mean(), 1),\n    \"Mean Image Height\": round(img_sizes[:,1].mean(), 1),\n    \"Mean Box Area\": round(np.mean(areas), 1),\n    \"Mean Aspect Ratio\": round(np.mean(ratios), 2)\n}\n\nsummary_df = pd.DataFrame(list(summary.items()), columns=[\"Metric\", \"Value\"])\nprint(\"\\n=== EDA Summary ===\")\nprint(summary_df.to_string(index=False))\n\n# Optionally save summary as CSV for your report\nsummary_df.to_csv(\"eda_summary.csv\", index=False)\nprint(\"\\n‚úÖ EDA summary saved as eda_summary.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T23:53:56.705040Z","iopub.execute_input":"2025-10-12T23:53:56.705338Z","iopub.status.idle":"2025-10-12T23:54:01.558565Z","shell.execute_reply.started":"2025-10-12T23:53:56.705317Z","shell.execute_reply":"2025-10-12T23:54:01.557924Z"}},"outputs":[],"execution_count":null}]}
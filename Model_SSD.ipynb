{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvBUvMOwr14E",
        "outputId": "7ced8c77-25e7-48fd-d2ef-0b7a4a8e23a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/COEN691/SSD_Project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create and go to a safe working folder inside your Drive\n",
        "%cd /content/drive/MyDrive\n",
        "!mkdir -p COEN691/SSD_Project\n",
        "%cd COEN691/SSD_Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NxGijkusZbf"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install albumentations pycocotools opencv-python tqdm matplotlib pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nwQiIMhsl4i",
        "outputId": "438a342a-8c60-4e27-ca14-16634dfcb270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: False\n",
            "Torch: 2.8.0+cu126 | Python: 3.12.12\n"
          ]
        }
      ],
      "source": [
        "import torch, platform\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Torch:\", torch.__version__, \"| Python:\", platform.python_version())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY5PSuYMxnqS"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/raw/Images data/raw/Annotation data/coco models/checkpoints src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK1EILE5x1hm",
        "outputId": "942e5322-7681-4b83-8704-f8ecc86917b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n02085620-Chihuahua\n",
            "n02085782-Japanese_spaniel\n",
            "n02085936-Maltese_dog\n",
            "n02086079-Pekinese\n",
            "n02086240-Shih-Tzu\n",
            "n02086646-Blenheim_spaniel\n",
            "n02086910-papillon\n",
            "n02087046-toy_terrier\n",
            "n02087394-Rhodesian_ridgeback\n",
            "n02088094-Afghan_hound\n",
            "n02085620-Chihuahua\n",
            "n02085782-Japanese_spaniel\n",
            "n02085936-Maltese_dog\n",
            "n02086079-Pekinese\n",
            "n02086240-Shih-Tzu\n",
            "n02086646-Blenheim_spaniel\n",
            "n02086910-papillon\n",
            "n02087046-toy_terrier\n",
            "n02087394-Rhodesian_ridgeback\n",
            "n02088094-Afghan_hound\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset archives (â‰ˆ 750 MB each)\n",
        "!wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
        "!wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar\n",
        "\n",
        "# Unpack them into our data/raw folder\n",
        "!tar -xf images.tar -C data/raw/\n",
        "!tar -xf annotation.tar -C data/raw/\n",
        "\n",
        "# Check the folders\n",
        "!ls data/raw/Images | head\n",
        "!ls data/raw/Annotation | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsLw6wlM7Xes",
        "outputId": "83bfb27d-489f-40ce-a51b-ac79a35186fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing src/prepare_dataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/prepare_dataset.py\n",
        "import os, json, random, xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "\n",
        "RAW_IMG_DIR = Path(\"data/raw/Images/\")\n",
        "RAW_ANN_DIR = Path(\"data/raw/Annotation/\")\n",
        "print(\"IMG DIR:\", RAW_IMG_DIR.resolve())\n",
        "print(\"ANN DIR:\", RAW_ANN_DIR.resolve())\n",
        "\n",
        "COCO_DIR    = Path(\"data/coco\"); COCO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def collect_pairs():\n",
        "    pairs = []\n",
        "    for breed in sorted(os.listdir(RAW_ANN_DIR)):\n",
        "        ann_dir = RAW_ANN_DIR/breed\n",
        "        img_dir = RAW_IMG_DIR/breed\n",
        "        if not ann_dir.is_dir():\n",
        "            continue\n",
        "        for xml in ann_dir.glob(\"*.xml\"):\n",
        "            img = img_dir/(xml.stem + \".jpg\")\n",
        "            if img.exists():\n",
        "                pairs.append((img, xml))\n",
        "    return pairs\n",
        "\n",
        "def parse_xml(xml_path):\n",
        "    root = ET.parse(xml_path).getroot()\n",
        "    bbox = root.find(\"object/bndbox\")\n",
        "    xmin = float(bbox.find(\"xmin\").text); ymin = float(bbox.find(\"ymin\").text)\n",
        "    xmax = float(bbox.find(\"xmax\").text); ymax = float(bbox.find(\"ymax\").text)\n",
        "    width = int(root.find(\"size/width\").text); height = int(root.find(\"size/height\").text)\n",
        "    x, y, w, h = xmin, ymin, xmax - xmin, ymax - ymin\n",
        "    return x, y, w, h, width, height\n",
        "\n",
        "def main():\n",
        "    pairs = collect_pairs()\n",
        "    random.seed(42); random.shuffle(pairs)\n",
        "    n = len(pairs)\n",
        "    train = pairs[:int(0.7*n)]\n",
        "    val   = pairs[int(0.7*n):int(0.85*n)]\n",
        "    test  = pairs[int(0.85*n):]\n",
        "\n",
        "    breeds = sorted([b for b in os.listdir(RAW_IMG_DIR) if (RAW_IMG_DIR/b).is_dir()])\n",
        "    cat_id = {name: i+1 for i, name in enumerate(breeds)}\n",
        "    categories = [{\"id\": i+1, \"name\": b, \"supercategory\": \"dog\"} for i, b in enumerate(breeds)]\n",
        "\n",
        "    def make_split(name, split_pairs):\n",
        "        images, annotations = [], []\n",
        "        img_id = 1; ann_id = 1\n",
        "        for img_path, xml_path in split_pairs:\n",
        "            x, y, w, h, W, H = parse_xml(xml_path)\n",
        "            images.append({\"id\": img_id, \"file_name\": str(img_path),\n",
        "                           \"width\": W, \"height\": H})\n",
        "            breed = xml_path.parent.name\n",
        "            annotations.append({\"id\": ann_id, \"image_id\": img_id,\n",
        "                                \"category_id\": cat_id[breed],\n",
        "                                \"bbox\": [x,y,w,h], \"area\": w*h, \"iscrowd\": 0})\n",
        "            img_id += 1; ann_id += 1\n",
        "        coco = {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
        "        with open(COCO_DIR/f\"{name}.json\",\"w\") as f: json.dump(coco,f)\n",
        "        print(f\"{name}: {len(images)} images, {len(annotations)} anns\")\n",
        "\n",
        "    make_split(\"train\", train); make_split(\"val\", val); make_split(\"test\", test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "851QdC1M8eWu",
        "outputId": "41a9332d-7470-41cc-e95e-b8755a4ab5c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current folder: /content/drive/MyDrive/COEN691/SSD_Project\n",
            "annotation.tar\tdata  images.tar  models  src\n",
            "coco  raw\n",
            "Annotation  Images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current folder:\", os.getcwd())\n",
        "!ls\n",
        "!ls data\n",
        "!ls data/raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdlvSnKL9Fwg",
        "outputId": "6884f0cb-4580-45e6-9961-eb2f0af5620a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'Images': No such file or directory\n",
            "mv: cannot stat 'Annotation': No such file or directory\n",
            "Annotation  Images\n",
            "n02085620-Chihuahua\n",
            "n02085782-Japanese_spaniel\n",
            "n02085936-Maltese_dog\n",
            "n02086079-Pekinese\n",
            "n02086240-Shih-Tzu\n",
            "n02086646-Blenheim_spaniel\n",
            "n02086910-papillon\n",
            "n02087046-toy_terrier\n",
            "n02087394-Rhodesian_ridgeback\n",
            "n02088094-Afghan_hound\n",
            "n02085620-Chihuahua\n",
            "n02085782-Japanese_spaniel\n",
            "n02085936-Maltese_dog\n",
            "n02086079-Pekinese\n",
            "n02086240-Shih-Tzu\n",
            "n02086646-Blenheim_spaniel\n",
            "n02086910-papillon\n",
            "n02087046-toy_terrier\n",
            "n02087394-Rhodesian_ridgeback\n",
            "n02088094-Afghan_hound\n"
          ]
        }
      ],
      "source": [
        "# Make sure the standard folders exist\n",
        "!mkdir -p data/raw\n",
        "\n",
        "# Move the top-level Images and Annotation into data/raw\n",
        "# (the \"|| true\" part just avoids crashing if they already exist)\n",
        "!mv Images data/raw/Images || true\n",
        "!mv Annotation data/raw/Annotation || true\n",
        "\n",
        "# Check the new structure\n",
        "!ls data/raw\n",
        "!ls data/raw/Images | head\n",
        "!ls data/raw/Annotation | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zd0Kdsc-RyP",
        "outputId": "c7366db4-3e00-4b93-eabd-4effc8d7842a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n02085620-Chihuahua\n",
            "n02085782-Japanese_spaniel\n",
            "n02085936-Maltese_dog\n",
            "n02086079-Pekinese\n",
            "n02086240-Shih-Tzu\n",
            "n02086646-Blenheim_spaniel\n",
            "n02086910-papillon\n",
            "n02087046-toy_terrier\n",
            "n02087394-Rhodesian_ridgeback\n",
            "n02088094-Afghan_hound\n",
            "n02085620-Chihuahua\n",
            "n02085782-Japanese_spaniel\n",
            "n02085936-Maltese_dog\n",
            "n02086079-Pekinese\n",
            "n02086240-Shih-Tzu\n",
            "n02086646-Blenheim_spaniel\n",
            "n02086910-papillon\n",
            "n02087046-toy_terrier\n",
            "n02087394-Rhodesian_ridgeback\n",
            "n02088094-Afghan_hound\n"
          ]
        }
      ],
      "source": [
        "!ls data/raw/Images | head\n",
        "!ls data/raw/Annotation | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBHYYZbFREu5",
        "outputId": "2ad67fc1-3bc0-4ea4-98c3-e7509804cb50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/COEN691/SSD_Project\n",
            "Current folder: /content/drive/MyDrive/COEN691/SSD_Project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/COEN691/SSD_Project\n",
        "import os\n",
        "print(\"Current folder:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r9rLCpgRfBy",
        "outputId": "1c333754-4084-4453-e85e-d8f87208356f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models\tsrc\n"
          ]
        }
      ],
      "source": [
        "!rm -rf data\n",
        "!rm -f images.tar annotation.tar\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK87VwZWSf0s",
        "outputId": "18332924-6254-4f34-adea-0d83fd30deed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/raw\n",
        "!ls data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TG6pPAvSpGZ",
        "outputId": "8805971b-c78b-43e2-ba7f-2950f2e999da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root  21M Oct 24  2011 annotation.tar\n",
            "-rw------- 1 root root 757M Oct 20  2011 images.tar\n"
          ]
        }
      ],
      "source": [
        "!wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar -O images.tar\n",
        "!wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar -O annotation.tar\n",
        "\n",
        "!ls -lh images.tar annotation.tar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYmFR3tvTFbA",
        "outputId": "61c1af9b-6a75-4f96-96c7-9a7bbec6aa34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotation  Images\n",
            "n02085620-Chihuahua\n",
            "n02085782-Japanese_spaniel\n",
            "n02085936-Maltese_dog\n",
            "n02086079-Pekinese\n",
            "n02086240-Shih-Tzu\n",
            "n02086646-Blenheim_spaniel\n",
            "n02086910-papillon\n",
            "n02087046-toy_terrier\n",
            "n02087394-Rhodesian_ridgeback\n",
            "n02088094-Afghan_hound\n",
            "n02085620-Chihuahua\n",
            "n02085782-Japanese_spaniel\n",
            "n02085936-Maltese_dog\n",
            "n02086079-Pekinese\n",
            "n02086240-Shih-Tzu\n",
            "n02086646-Blenheim_spaniel\n",
            "n02086910-papillon\n",
            "n02087046-toy_terrier\n",
            "n02087394-Rhodesian_ridgeback\n",
            "n02088094-Afghan_hound\n"
          ]
        }
      ],
      "source": [
        "# This will create data/raw/Images and data/raw/Annotation\n",
        "!tar -xf images.tar -C data/raw/\n",
        "!tar -xf annotation.tar -C data/raw/\n",
        "\n",
        "!ls data/raw\n",
        "!ls data/raw/Images | head\n",
        "!ls data/raw/Annotation | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAb4fJkFYH-a",
        "outputId": "237484e5-d4c0-4707-cd88-3ec73030dbcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_10074\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_10131\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_10621\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_1073\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_10976\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_11140\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_11238\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_11258\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_11337\n",
            "data/raw/Annotation/n02085620-Chihuahua/n02085620_11477\n"
          ]
        }
      ],
      "source": [
        "!find data/raw/Annotation -maxdepth 3 -type f | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNEVk9ckbpj1",
        "outputId": "59c91136-7766-437e-894f-46b389b29b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/prepare_dataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/prepare_dataset.py\n",
        "import os, json, random, xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "\n",
        "RAW_IMG_DIR = Path(\"data/raw/Images\")\n",
        "RAW_ANN_DIR = Path(\"data/raw/Annotation\")\n",
        "COCO_DIR    = Path(\"data/coco\"); COCO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def debug_check_dirs():\n",
        "    print(\"IMG DIR:\", RAW_IMG_DIR.resolve())\n",
        "    print(\"ANN DIR:\", RAW_ANN_DIR.resolve())\n",
        "    print(\"IMG DIR exists:\", RAW_IMG_DIR.exists())\n",
        "    print(\"ANN DIR exists:\", RAW_ANN_DIR.exists())\n",
        "    if RAW_IMG_DIR.exists():\n",
        "        breeds_img = [d.name for d in RAW_IMG_DIR.iterdir() if d.is_dir()]\n",
        "        print(\"Sample image breeds:\", breeds_img[:5])\n",
        "    if RAW_ANN_DIR.exists():\n",
        "        breeds_ann = [d.name for d in RAW_ANN_DIR.iterdir() if d.is_dir()]\n",
        "        print(\"Sample annot breeds:\", breeds_ann[:5])\n",
        "\n",
        "def collect_pairs():\n",
        "    pairs = []\n",
        "    missing_img = 0\n",
        "    total_xml = 0\n",
        "\n",
        "    # ðŸ” NEW: treat *any file* under Annotation as an XML (Stanford Dogs has no .xml extension)\n",
        "    ann_files = [p for p in RAW_ANN_DIR.rglob(\"*\") if p.is_file()]\n",
        "    if not ann_files:\n",
        "        print(\"[ERROR] No annotation files found anywhere under\", RAW_ANN_DIR)\n",
        "        return []\n",
        "\n",
        "    print(f\"[INFO] Total annotation files found (recursive): {len(ann_files)}\")\n",
        "\n",
        "    for ann_path in ann_files:\n",
        "        total_xml += 1\n",
        "        # Breed folder = top-level folder under Annotation (e.g., n02085620-Chihuahua)\n",
        "        rel = ann_path.relative_to(RAW_ANN_DIR)\n",
        "        breed_folder = rel.parts[0]\n",
        "\n",
        "        img_dir = RAW_IMG_DIR / breed_folder\n",
        "\n",
        "        stem = ann_path.name  # e.g., \"n02085620_10074\" (no extension)\n",
        "        img = img_dir / (stem + \".jpg\")\n",
        "        if not img.exists():\n",
        "            img_alt = img_dir / (stem + \".JPEG\")\n",
        "            if img_alt.exists():\n",
        "                img = img_alt\n",
        "            else:\n",
        "                missing_img += 1\n",
        "                # print(f\"[MISS] No image for annotation: {ann_path}\")\n",
        "                continue\n",
        "\n",
        "        pairs.append((img, ann_path))\n",
        "\n",
        "    print(f\"[SUMMARY] Total annotation files scanned: {total_xml}\")\n",
        "    print(f\"[SUMMARY] Valid (img+ann) pairs: {len(pairs)}\")\n",
        "    print(f\"[SUMMARY] Missing images for annotations: {missing_img}\")\n",
        "    return pairs\n",
        "\n",
        "def parse_xml(xml_path):\n",
        "    # Even without .xml extension, the content is XML, so ET.parse works fine\n",
        "    root = ET.parse(xml_path).getroot()\n",
        "    bbox = root.find(\"object/bndbox\")\n",
        "    xmin = float(bbox.find(\"xmin\").text); ymin = float(bbox.find(\"ymin\").text)\n",
        "    xmax = float(bbox.find(\"xmax\").text); ymax = float(bbox.find(\"ymax\").text)\n",
        "    width = int(root.find(\"size/width\").text); height = int(root.find(\"size/height\").text)\n",
        "    x, y, w, h = xmin, ymin, xmax - xmin, ymax - ymin\n",
        "    return x, y, w, h, width, height\n",
        "\n",
        "def main():\n",
        "    debug_check_dirs()\n",
        "\n",
        "    pairs = collect_pairs()\n",
        "    if not pairs:\n",
        "        print(\"[ERROR] No (image, annotation) pairs found after recursive search.\")\n",
        "        return\n",
        "\n",
        "    random.seed(42); random.shuffle(pairs)\n",
        "    n = len(pairs)\n",
        "    train = pairs[:int(0.7*n)]\n",
        "    val   = pairs[int(0.7*n):int(0.85*n)]\n",
        "    test  = pairs[int(0.85*n):]\n",
        "\n",
        "    print(f\"[SPLIT] Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
        "\n",
        "    # categories from image folders\n",
        "    breeds = sorted([b for b in os.listdir(RAW_IMG_DIR) if (RAW_IMG_DIR/b).is_dir()])\n",
        "    cat_id = {name: i+1 for i, name in enumerate(breeds)}\n",
        "    categories = [{\"id\": i+1, \"name\": b, \"supercategory\": \"dog\"} for i, b in enumerate(breeds)]\n",
        "\n",
        "    def make_split(name, split_pairs):\n",
        "        images, annotations = [], []\n",
        "        img_id = 1; ann_id = 1\n",
        "        for img_path, ann_path in split_pairs:\n",
        "            x, y, w, h, W, H = parse_xml(ann_path)\n",
        "            images.append({\n",
        "                \"id\": img_id,\n",
        "                \"file_name\": str(img_path).replace(\"\\\\\", \"/\"),\n",
        "                \"width\": W,\n",
        "                \"height\": H\n",
        "            })\n",
        "            breed_folder = ann_path.relative_to(RAW_ANN_DIR).parts[0]\n",
        "            annotations.append({\n",
        "                \"id\": ann_id,\n",
        "                \"image_id\": img_id,\n",
        "                \"category_id\": cat_id[breed_folder],\n",
        "                \"bbox\": [x, y, w, h],\n",
        "                \"area\": w*h,\n",
        "                \"iscrowd\": 0\n",
        "            })\n",
        "            img_id += 1; ann_id += 1\n",
        "\n",
        "        coco = {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
        "        out_path = COCO_DIR / f\"{name}.json\"\n",
        "        with open(out_path, \"w\") as f:\n",
        "            json.dump(coco, f)\n",
        "        print(f\"[WRITE] {name}: {len(images)} images, {len(annotations)} anns -> {out_path}\")\n",
        "\n",
        "    make_split(\"train\", train)\n",
        "    make_split(\"val\", val)\n",
        "    make_split(\"test\", test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yaaz8sTAbwTd",
        "outputId": "b092776a-238d-4b43-cea9-265072c65db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMG DIR: /content/drive/MyDrive/COEN691/SSD_Project/data/raw/Images\n",
            "ANN DIR: /content/drive/MyDrive/COEN691/SSD_Project/data/raw/Annotation\n",
            "IMG DIR exists: True\n",
            "ANN DIR exists: True\n",
            "Sample image breeds: ['n02085620-Chihuahua', 'n02085782-Japanese_spaniel', 'n02085936-Maltese_dog', 'n02086079-Pekinese', 'n02086240-Shih-Tzu']\n",
            "Sample annot breeds: ['n02085620-Chihuahua', 'n02085782-Japanese_spaniel', 'n02085936-Maltese_dog', 'n02086079-Pekinese', 'n02086240-Shih-Tzu']\n",
            "[INFO] Total annotation files found (recursive): 20580\n",
            "[SUMMARY] Total annotation files scanned: 20580\n",
            "[SUMMARY] Valid (img+ann) pairs: 20580\n",
            "[SUMMARY] Missing images for annotations: 0\n",
            "[SPLIT] Train: 14405, Val: 3088, Test: 3087\n",
            "[WRITE] train: 14405 images, 14405 anns -> data/coco/train.json\n",
            "[WRITE] val: 3088 images, 3088 anns -> data/coco/val.json\n",
            "[WRITE] test: 3087 images, 3087 anns -> data/coco/test.json\n"
          ]
        }
      ],
      "source": [
        "!python src/prepare_dataset.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZU5gkfncmYs",
        "outputId": "47e5a81d-6d2b-40b9-cad0-bfb72b36c5a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 4.7M\n",
            "-rw------- 1 root root 719K Nov 13 18:19 test.json\n",
            "-rw------- 1 root root 3.3M Nov 13 18:18 train.json\n",
            "-rw------- 1 root root 720K Nov 13 18:18 val.json\n"
          ]
        }
      ],
      "source": [
        "!ls -lh data/coco\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czNWW3O0O_0p",
        "outputId": "613c8b3e-06ad-4c13-f601-8596f6356b43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 'n02085620-Chihuahua'),\n",
              " (2, 'n02085782-Japanese_spaniel'),\n",
              " (3, 'n02085936-Maltese_dog'),\n",
              " (4, 'n02086079-Pekinese'),\n",
              " (5, 'n02086240-Shih-Tzu'),\n",
              " (6, 'n02086646-Blenheim_spaniel'),\n",
              " (7, 'n02086910-papillon'),\n",
              " (8, 'n02087046-toy_terrier'),\n",
              " (9, 'n02087394-Rhodesian_ridgeback'),\n",
              " (10, 'n02088094-Afghan_hound')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load your COCO training annotation file\n",
        "with open(\"data/coco/train.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create mapping from numeric id to breed name\n",
        "id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
        "\n",
        "# Show first few entries\n",
        "list(id_to_name.items())[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s43gGUGA1fih",
        "outputId": "9ce8947e-2941-44e9-8c77-b3f6ac069ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/COEN691/SSD_Project\n",
            "annotation.tar\tmodels\t\t      ssd_detection_example.jpg\n",
            "data\t\tsrc\t\t      ssd_labeled_bbox.jpg\n",
            "images.tar\tssd_bbox_example.jpg  ssd_labeled_clean.jpg\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/COEN691/SSD_Project\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0SFSxDa2X1x",
        "outputId": "0cf4ee7d-8293-4edd-ce1e-f6317ed1e9c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: src/ (stored 0%)\n",
            "  adding: src/prepare_dataset.py (deflated 63%)\n",
            "  adding: src/__pycache__/ (stored 0%)\n",
            "  adding: src/__pycache__/train_ssd.cpython-312.pyc (deflated 39%)\n",
            "  adding: src/test_ssd.py (deflated 53%)\n",
            "  adding: src/visualize_ssd.py (deflated 51%)\n",
            "  adding: src/train_ssd.py (deflated 58%)\n",
            "  adding: data/coco/ (stored 0%)\n",
            "  adding: data/coco/train.json (deflated 85%)\n",
            "  adding: data/coco/val.json (deflated 85%)\n",
            "  adding: data/coco/test.json (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r src.zip src\n",
        "!zip -r data_coco.zip data/coco\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iZXmUXG2kBj",
        "outputId": "ee16922f-485e-44e9-975b-71582ef1f2bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: models/checkpoints/ (stored 0%)\n",
            "  adding: models/checkpoints/ssd_best.pth (deflated 8%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r checkpoints.zip models/checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNjxd5Ii2uJO",
        "outputId": "b6e98adb-bad1-44c9-a1d2-8fd9ce0453d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: ssd_detection_example.jpg (deflated 6%)\n"
          ]
        }
      ],
      "source": [
        "!zip ssd_detection_example.zip ssd_detection_example.jpg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPiRVOB_27w4",
        "outputId": "a108fee4-f4c8-405f-8cd4-45fd402042ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 140M Nov 16 08:49 checkpoints.zip\n",
            "-rw------- 1 root root 704K Nov 16 08:48 data_coco.zip\n",
            "-rw------- 1 root root  11K Nov 16 08:48 src.zip\n",
            "-rw------- 1 root root 112K Nov 16 08:49 ssd_detection_example.zip\n"
          ]
        }
      ],
      "source": [
        "!ls -lh *.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_GbjQIbD-wv",
        "outputId": "f989bb1d-9d68-4f81-d913-90061566853e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/COEN691/SSD_Project\n",
            "eval_ssd_anyhit.py  prepare_dataset.py\ttest_ssd.py   visualize_ssd.py\n",
            "eval_ssd_top1.py    __pycache__\t\ttrain_ssd.py\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/COEN691/SSD_Project\n",
        "!ls src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLcP4EzRepVh",
        "outputId": "34510154-be05-4aef-d865-ac5464f67564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device: cuda\n",
            "Number of dog breeds (classes): 120\n",
            "loading annotations into memory...\n",
            "Done (t=0.05s)\n",
            "creating index...\n",
            "index created!\n",
            "Train samples: 14405\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [36:10<00:00,  1.66it/s, loss=9.702]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: avg train loss = 15.6374\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:36<00:00,  5.66it/s, loss=10.071]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: avg train loss = 8.2723\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=6.278]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: avg train loss = 8.1834\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=6.815]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: avg train loss = 8.1427\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=7.990]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: avg train loss = 8.1240\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=6.325]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: avg train loss = 8.1145\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.66it/s, loss=6.271]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: avg train loss = 8.1098\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.66it/s, loss=6.155]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: avg train loss = 8.1064\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=13.185]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: avg train loss = 8.0986\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=6.571]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: avg train loss = 8.0938\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:36<00:00,  5.66it/s, loss=6.476]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: avg train loss = 8.0629\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=9.453]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: avg train loss = 8.0604\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=9.317]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: avg train loss = 8.0580\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:35<00:00,  5.67it/s, loss=6.544]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: avg train loss = 8.0582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:32<00:00,  5.70it/s, loss=6.784]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: avg train loss = 8.0615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:32<00:00,  5.70it/s, loss=6.621]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: avg train loss = 8.0594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:32<00:00,  5.70it/s, loss=6.209]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: avg train loss = 8.0589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:32<00:00,  5.70it/s, loss=6.854]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: avg train loss = 8.0572\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:36<00:00,  5.66it/s, loss=6.189]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: avg train loss = 8.0579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20 (train): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3602/3602 [10:33<00:00,  5.68it/s, loss=10.197]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: avg train loss = 8.0559\n",
            "âœ… Saved best model so far to models/checkpoints/ssd_best.pth\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.models.detection import ssd300_vgg16\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 4          # keep small for stability\n",
        "EPOCHS = 20             # you can change to 1 or 3\n",
        "LR = 1e-4               # small LR to avoid exploding loss\n",
        "\n",
        "class DogsCoco(CocoDetection):\n",
        "    \"\"\"\n",
        "    Simple dataset:\n",
        "    - uses CocoDetection to get (image, target)\n",
        "    - converts COCO bbox [x,y,w,h] -> [x_min,y_min,x_max,y_max]\n",
        "    - returns image tensor + target dict for SSD\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ann_file):\n",
        "        super().__init__(root=\".\", annFile=ann_file, transforms=None)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, annotations_list = super().__getitem__(idx)\n",
        "        img_np = img.convert(\"RGB\")\n",
        "        W, H = img_np.size # Image width and height from PIL Image\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for ann in annotations_list:\n",
        "            x_orig, y_orig, w_orig, h_orig = ann['bbox'] # COCO format: [x, y, width, height]\n",
        "\n",
        "            # Convert to [x_min, y_min, x_max, y_max] format\n",
        "            x_min = float(x_orig)\n",
        "            y_min = float(y_orig)\n",
        "            x_max = float(x_orig + w_orig)\n",
        "            y_max = float(y_orig + h_orig)\n",
        "\n",
        "            # Clamp coordinates to be within image boundaries [0, W] and [0, H]\n",
        "            x_min = max(0.0, x_min)\n",
        "            y_min = max(0.0, y_min)\n",
        "            x_max = min(float(W), x_max)\n",
        "            y_max = min(float(H), y_max)\n",
        "\n",
        "            # Re-check for valid dimensions after clamping\n",
        "            # If after clamping, width or height is non-positive, skip this box.\n",
        "            if x_max <= x_min or y_max <= y_min:\n",
        "                continue\n",
        "\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        # If no valid boxes remain after filtering, add a tiny dummy box\n",
        "        if not boxes:\n",
        "            x_min_dummy = W * 0.4\n",
        "            y_min_dummy = H * 0.4\n",
        "            x_max_dummy = W * 0.6\n",
        "            y_max_dummy = H * 0.6\n",
        "            # Ensure dummy box has positive width and height\n",
        "            # This condition ensures that if W or H is 0, the dummy box is still valid if possible\n",
        "            if x_max_dummy <= x_min_dummy: x_max_dummy = x_min_dummy + 1 # Add 1 to ensure positive width\n",
        "            if y_max_dummy <= y_min_dummy: y_max_dummy = y_min_dummy + 1 # Add 1 to ensure positive height\n",
        "            boxes.append([x_min_dummy, y_min_dummy, x_max_dummy, y_max_dummy])\n",
        "            labels.append(1) # Assign to an arbitrary valid class ID (e.g., 1)\n",
        "\n",
        "        # Convert image to tensor in [0,1], float32\n",
        "        img_t = TF.to_tensor(img_np) # Converts to float32 in [0,1]\n",
        "        boxes_t = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels_t = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target_dict = {\n",
        "            \"boxes\": boxes_t,\n",
        "            \"labels\": labels_t,\n",
        "        }\n",
        "        return img_t, target_dict\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = list(zip(*batch))\n",
        "    return list(imgs), list(targets)\n",
        "\n",
        "def num_classes_from_json(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    return len(data[\"categories\"])\n",
        "\n",
        "def main():\n",
        "    print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "    train_json = \"data/coco/train.json\"\n",
        "\n",
        "    num_classes = num_classes_from_json(train_json)\n",
        "    print(\"Number of dog breeds (classes):\", num_classes)\n",
        "\n",
        "    train_ds = DogsCoco(train_json)\n",
        "    print(\"Train samples:\", len(train_ds))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=False,\n",
        "    )\n",
        "\n",
        "    ckpt_dir = Path(\"models/checkpoints\")\n",
        "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ckpt_path = ckpt_dir / \"ssd_best.pth\"\n",
        "\n",
        "    # Model\n",
        "    model = ssd300_vgg16(weights=None, num_classes=num_classes + 1)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params, lr=LR, momentum=0.9, weight_decay=5e-4\n",
        "    )\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1) # Moved scheduler.step() to after loss calculation, but before saving best model. Also set step_size = 10, instead of 1.\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} (train)\")\n",
        "\n",
        "        for images, targets in pbar:\n",
        "            images = [img.to(DEVICE) for img in images]\n",
        "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            loss = sum(loss_dict.values())\n",
        "            loss_val = loss.item()\n",
        "\n",
        "            # Skip bad batches with NaN/Inf loss\n",
        "            if not math.isfinite(loss_val):\n",
        "                print(f\"âš ï¸  Skipping batch with non-finite loss: {loss_val}\")\n",
        "                continue\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss_val\n",
        "            pbar.set_postfix(loss=f\"{loss_val:.3f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_train_loss = running_loss / max(1, len(train_loader))\n",
        "        print(f\"Epoch {epoch}: avg train loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "        # save model at end of each epoch if it's the best so far\n",
        "        if avg_train_loss < best_loss:\n",
        "            best_loss = avg_train_loss\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            print(f\"âœ… Saved best model so far to {ckpt_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo7Y3tan2BFk",
        "outputId": "15b07a88-ec7c-46bb-be16-ec634cc37e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing src/eval_ssd_anyhit.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/eval_ssd_anyhit.py\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.models.detection import ssd300_vgg16\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "class DogsCocoVal(CocoDetection):\n",
        "    \"\"\"\n",
        "    COCO dataset wrapper for Stanford Dogs validation set.\n",
        "    We only use the first annotation's category_id as the ground-truth breed label.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_root, ann_file):\n",
        "        super().__init__(root=img_root, annFile=ann_file, transforms=None)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super().__getitem__(idx)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img_t = TF.to_tensor(img)  # [3,H,W], float32 in [0,1]\n",
        "\n",
        "        # target is a list of annotations for that image (dicts)\n",
        "        if len(target) == 0:\n",
        "            gt_label = -1  # no annotation\n",
        "        else:\n",
        "            # use first dog's category_id as GT breed label\n",
        "            gt_label = int(target[0][\"category_id\"])\n",
        "\n",
        "        return img_t, gt_label\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, labels = zip(*batch)\n",
        "    return list(imgs), list(labels)\n",
        "\n",
        "\n",
        "def main():\n",
        "    ann_path = \"data/coco/val.json\"\n",
        "    img_root = \".\"\n",
        "\n",
        "    # read categories to know how many classes we have\n",
        "    with open(ann_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    categories = data[\"categories\"]\n",
        "    num_classes = len(categories)\n",
        "    print(f\"Number of breeds (classes): {num_classes}\")\n",
        "\n",
        "    # dataset & loader\n",
        "    ds = DogsCocoVal(img_root=img_root, ann_file=ann_path)\n",
        "    loader = DataLoader(\n",
        "        ds,\n",
        "        batch_size=4,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "    )\n",
        "    print(f\"Validation samples: {len(ds)}\")\n",
        "\n",
        "    # load model\n",
        "    model = ssd300_vgg16(weights=None, num_classes=num_classes + 1)\n",
        "    ckpt_path = \"models/checkpoints/ssd_best.pth\"\n",
        "    print(f\"Loading checkpoint from: {ckpt_path}\")\n",
        "    state_dict = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, gt_labels in loader:\n",
        "            imgs = [img.to(DEVICE) for img in imgs]\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            for out, gt in zip(outputs, gt_labels):\n",
        "                gt = int(gt)\n",
        "                if gt == -1:\n",
        "                    continue  # skip if no GT\n",
        "\n",
        "                labels = out[\"labels\"].cpu()\n",
        "\n",
        "                # relaxed metric: if *any* predicted label equals GT, call it correct\n",
        "                if (labels == gt).any():\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    print(\"\\nRelaxed metric: does ANY predicted label match the GT breed?\")\n",
        "    print(f\"Accuracy_any_hit = {acc:.4f} ({correct}/{total})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bknyV88-2IYM",
        "outputId": "ad012143-4a53-4dfe-91f0-0b3460a3827f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/COEN691/SSD_Project\n",
            "Number of breeds (classes): 120\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Validation samples: 3088\n",
            "Loading checkpoint from: models/checkpoints/ssd_best.pth\n",
            "\n",
            "Relaxed metric: does ANY predicted label match the GT breed?\n",
            "Accuracy_any_hit = 0.0081 (25/3088)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/COEN691/SSD_Project\n",
        "\n",
        "!python src/eval_ssd_anyhit.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06itpR_8T0VP",
        "outputId": "bec5338c-0935-4fc9-af49-ed72dce57e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/COEN691/SSD_Project/models/checkpoints/ssd_best.pth\n"
          ]
        }
      ],
      "source": [
        "!find /content/drive/MyDrive/COEN691/SSD_Project -name \"ssd_best.pth\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhwITgV2MtsG",
        "outputId": "2531b126-0725-482c-f6e4-6f32c12fe3ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing src/test_ssd.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/test_ssd.py\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.models.detection import ssd300_vgg16\n",
        "from PIL import Image\n",
        "\n",
        "from train_ssd import num_classes_from_json  # reuse helper\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def main():\n",
        "    # 1. Load one image path from train.json\n",
        "    train_json = \"data/coco/train.json\"\n",
        "    with open(train_json, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    first_img = data[\"images\"][0]\n",
        "    img_path = first_img[\"file_name\"]\n",
        "    print(\"Using image:\", img_path)\n",
        "\n",
        "    # 2. Load image\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_t = TF.to_tensor(img).to(DEVICE).unsqueeze(0)  # [1,3,H,W]\n",
        "\n",
        "    # 3. Build model and load weights\n",
        "    num_classes = num_classes_from_json(train_json)\n",
        "    model = ssd300_vgg16(weights=None, num_classes=num_classes + 1)\n",
        "    ckpt_path = Path(\"models/checkpoints/ssd_best.pth\")\n",
        "    state_dict = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    # 4. Run prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_t)\n",
        "\n",
        "    preds = outputs[0]\n",
        "    boxes = preds[\"boxes\"]\n",
        "    labels = preds[\"labels\"]\n",
        "    scores = preds[\"scores\"]\n",
        "\n",
        "    # 5. Print top 5 predictions\n",
        "    print(\"Top 5 detections:\")\n",
        "    for i in range(min(5, len(boxes))):\n",
        "        breed_name = id_to_name.get(labels[i].item(), \"Unknown\")\n",
        "print(f\"- box={boxes[i].cpu().numpy()}, breed={breed_name}, score={scores[i].item():.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FiOKo5SP75A",
        "outputId": "2935a117-95a9-4b53-b58a-2106d7e5ca60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using image: data/raw/Images/n02106382-Bouvier_des_Flandres/n02106382_4243.jpg\n",
            "Top 5 detections:\n",
            "- box=[ 60.16299   34.112934 518.9856   322.909   ], label_id=5, score=0.012\n",
            "- box=[ 60.16299   34.112934 518.9856   322.909   ], label_id=109, score=0.011\n",
            "- box=[ 63.47534   33.976807 512.04926  320.42667 ], label_id=89, score=0.011\n",
            "- box=[ 60.16299   34.112934 518.9856   322.909   ], label_id=27, score=0.011\n",
            "- box=[ 60.16299   34.112934 518.9856   322.909   ], label_id=3, score=0.011\n"
          ]
        }
      ],
      "source": [
        "!python src/test_ssd.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ioAHe2zZD_7",
        "outputId": "6fe75449-ab26-4476-a596-bc578175cbe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/COEN691/SSD_Project\n",
            "/content/drive/MyDrive/COEN691/SSD_Project\n",
            "annotation.tar\t images.tar  ssd_bbox_example.jpg\tssd_labeled_clean.jpg\n",
            "checkpoints.zip  models      ssd_detection_example.jpg\n",
            "data\t\t src\t     ssd_detection_example.zip\n",
            "data_coco.zip\t src.zip     ssd_labeled_bbox.jpg\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/COEN691/SSD_Project\n",
        "!pwd\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3YaJ6qPZgy9",
        "outputId": "d46358cc-5084-4482-b36e-28c0cdaeb43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: src/ (stored 0%)\n",
            "updating: src/prepare_dataset.py (deflated 63%)\n",
            "updating: src/__pycache__/ (stored 0%)\n",
            "updating: src/__pycache__/train_ssd.cpython-312.pyc (deflated 39%)\n",
            "updating: src/test_ssd.py (deflated 53%)\n",
            "updating: src/visualize_ssd.py (deflated 51%)\n",
            "updating: src/train_ssd.py (deflated 58%)\n",
            "updating: src/eval_ssd_top1.py (deflated 59%)\n",
            "updating: src/eval_ssd_anyhit.py (deflated 57%)\n",
            "updating: models/checkpoints/ssd_best.pth (deflated 8%)\n",
            "updating: data/coco/train.json (deflated 85%)\n",
            "updating: data/coco/val.json (deflated 85%)\n",
            "updating: data/coco/test.json (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r ssd_artifacts.zip \\\n",
        "  src \\\n",
        "  models/checkpoints/ssd_best.pth \\\n",
        "  data/coco/train.json \\\n",
        "  data/coco/val.json \\\n",
        "  data/coco/test.json\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
